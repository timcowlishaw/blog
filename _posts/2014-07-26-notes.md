---
layout: post
title: "Notes: 17th - 25th July"
---

My plan for producing daily notes has fallen by the wayside a little bit - I've
realised it's actually quite a large task to take on at the end of each day.
Also, I don't feel it gives quite enough time for reflection before writing, so
therefore I've been keeping rough notes every day, which I'll write up properly
every week or so, which seems like a more natural rhythm.

## Javascript, D3, Reactivity
Continuing the javascript datavis work I mentioned in my last set of notes, while on the whole I like working with d3, there are occasions when it doesn't work exactly as I expect, particulary around reflecting changes to bound data in the visualisation. Thinking about this a little, it seems like my expectation was that data-bound d3 visualisation would work in a reactive manner - responding dynamically to changes in the underlying data, which doesn't quite match up with the reality (you explicitly bind the visualisation to a new dataset, and d3 then works out how to update the visualisation based on the diff of the two datasets). Understanding this, I've basically got no problems any more, although I've noticed that a couple of people have produced [reactive][1] [extensions][2] to d3 which might be worth checking out at some point.

## Remedial Real Anaysis
I got hold of a copy of [Baby Rudin][25] and have started working through it
slowly. I've found [Rudinium][26] to be a great help here, and particularly the
Koans - a series of small quiz questions on the text of the book. The Koans are
less involved than the exercises in the book itself, and are a really
lightweight way of testing your understanding as you go along. Also, given that
I haven't found time to sit down and watch the [video lectures][27] from start
to finish, it provides a really useful index into the related passage of video
for each given topic, fantastic for gaining extra background on tricky concepts.

## Machine learning adventures - Dirichlet Processes and Factorization Machines
I've been trying to get through this [series of blog posts][3] on Dirichlet
Process Mixture Models for a while now, and have been getting stuck fairly early on in the
series, due to being unfamiliar with a few of the basics. To remedy this, I've
written up a very very gentle tutorial on the Dirichlet Process, based on my own
(very slow and deliberate) reading around the subject, which I'll have finished
shortly.

I was also introduced to [Factorization machines][4] this week (on
a recommendation by [Yves][5] of [this paper][6]), which are a really really
clever general framework for describing factorization models. What makes them
especially cool is that they can be used for regression, classification, and
ranking tasks, allow parameter estimation under very sparse data, and can be
computed in time linear in the dimension of the feature vector, while still
modelling interactions between variables of arbitrary order.

These properties (especially the handling of sparsity) suggests that they may be
very well suited for text-classification tasks. Also the fact that when
used as a classifier, the model outputs a measure of confidence in the
classification, means that it'd be a suitable candidate to adapt to an
active learning approach using [uncertainty sampling][7]. These properties mean
that I suspect it'll be a really interesting area to look into as part of my
[PhD proposal][8].

The paper itself is incredibly well-written - it's one of these cases (of which
I've come across a fair few) where it's the sheer _simplicity_ of the topic that
makes it initially hard to grasp, rather than any hidden complexity. It's
a really beautiful moment when you realise that there's no hidden complexity
waiting to trip you up, and the brilliantly simple model in front of you is
all that's required to solve a complicated problem so elegantly.

The model (as well as being beautifuly simple), can also be optimized directly
by gradient descent, which to my mind, makes it an interesting candidate for
running on [Spark][9], using the gradient descent routine in [MLlib][10]. I'm
going to have a go at that today!

## Programming, mathematics, and ways of learning.
My experience learning about Factorization Machines above is a specific
example of something I've been doing a lot recently when trying to learn new mathematical
or theoretical CS concepts: namely attempting to write a program that implements
them. This isn't a new or interesting teqchnique, but it's one I've found
valuable, and I've become interested in both the mechanism by which this helps
me absorb knowledge more effectively, and the specific properties of
a programming language that make it amenable to this sort of activity.

On the first point, the idea that practice and repetition is important to
becoming a better programmer isn't a new one - Dave Thomas's [popular
adapatation][11] of the Martial Arts concept of [_kata_][12] is a great example of
this. However, to my mind at least, Katas are all about internalising knowledge
and skill - turning them from something conscious into something more like an
unconcsious reflex. My experience of using implementation to better understand
something abstract is almost the opposite; it's a deliberate attempt to surface
my intuition about a topic and examine it deliberately and critically, by
turning the abstract form of an algorithm or mathematical model into a concrete,
running, code example.

With this in mind, I came across the concept of [Experiential learning][13],
popularized by a guy called David A. Kolb, which seems to describe my experience
rather well. Kolb's model of experiential learning describes a four stage cycle
of Application -> Experience -> Reflection -> Generalisation and back again,
which explicitly acknowledges that the conscious, deliberate experience of
applying some abstract knowledge can reinforce and deepend our knowledge of the
abstract, general concept that informs it.

To my mind, this experiential learning cycle mirrors the red-green-refactor
cycle of TDD. We describe our knowledge of a requirement as a test, actively
intervene by writing code to make that test pass, making the abstract
requirement into a concrete implementation, then reflect on this experience and
generalise our understanding of the problem through a process of refactoring.

This brings me nicely to seccond question, on the properties of a programming
language that encourages and supports this style of learning. We want to be able
to easily express our abstract, high-level knowledge about some mathematical
model or algorithm, use this high-level specification to drive out a concrete
implementation, and then use the insight we've gained from this process to
further refine and generalise our knowledge of the abstract model. For me, this
process cries out for an expressive, statically-checked type system, much like
Scala's.

I've a vague plan for a conference talk based on this, taking an interesting
paper, trying to implement it, and seeing what we learn about both the contents
of the paper and the process itself along the way. With that in mind, I'm
working on a proposal for [Scala Exchange][14] in December, through Underscore
Consulting's [new speaker program][15]. I don't know if they'll go for it, but if
they do it could be fun.

## Reading!
 A few other odds and ends I've read this week and found useful or enjoyed:

  - Aslak Helles&oslash;y's post on why cucumber is the [world's most
   misunderstood collaboration software][31] -  a really succinct outlin of
the benefits of BDD (and cucumber specifically) written succinctly and clearly.
Worth bookmarking for referring non-technical collaborators to!
  - [Some interesting benchmarks][32] of running Pylearn2 deep learning models on
   Amazon EC2 GPU compute instances. Also a useful record of how to get Pylearn2
or Theano running on there if I ever need to do anything like that.
  - A great post from Cathy O'Neill on [how microeconomists think][33], and in
   particular the role of mechanisms and causality in microeconomic thinking.
It's a nice illustration of the limitations of the type of descriptive stats
that makes up most 'big data' analysis, and offers great insight in how to build
upon this sort of descriptive analysis in order to better understand the
mechanisms at work (and therefore make better decisions and interventions) in
a principled, evidence-driven way.
  - A really [useful reference][34] on best practices around structuring and
   distributing python projects.

[1]: https://github.com/vogievetsky/DVL
[2]: http://eng.wealthfront.com/2013/04/reactive-charts-with-d3-and-reactivejs.html
[3]: http://blog.datumbox.com/overview-of-cluster-analysis-and-dirichlet-process-mixture-models/
[4]: http://www.libfm.org/
[5]: http://raimond.me.uk/
[6]: http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf
[7]: http://dl.acm.org/citation.cfm?id=188495
[8]: http://www.timcowlishaw.co.uk/files/proposal.pdf
[9]: https://spark.apache.org/
[10]: http://spark.apache.org/docs/latest/mllib-guide.html
[11]: http://codekata.com/
[12]: http://en.wikipedia.org/wiki/Kata
[13]: http://infed.org/mobi/david-a-kolb-on-experiential-learning/
[14]: https://skillsmatter.com/conferences/1948-scala-exchange-2014
[15]: http://underscoreconsulting.com/blog/posts/2014/06/30/underscores-new-speaker-program.html
[25]: http://www.goodreads.com/book/show/292079.Principles_of_Mathematical_Analysis
[26]: http://rudinium.herokuapp.com/
[27]: https://www.youtube.com/playlist?list=PL04BA7A9EB907EDAF
[31]: https://cucumber.pro/blog/2014/03/03/the-worlds-most-misunderstood-collaboration-tool.html
[32]: http://www.kurtsp.com/deep-learning-in-python-with-pylearn2-and-amazon-ec2.html
[33]: http://mathbabe.org/2014/07/23/how-to-think-like-a-microeconomist/
[34]: http://www.jeffknupp.com/blog/2013/08/16/open-sourcing-a-python-project-the-right-way/
